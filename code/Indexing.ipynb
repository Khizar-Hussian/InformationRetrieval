{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing\n",
    "### This Notebook contains example of creating index using simple algorithm\n",
    "\n",
    "Author: Noshaba Nasir\n",
    "\n",
    "Created on: 24/3/2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1.txt', '2.txt']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get document list\n",
    "path= dirname(abspath(os.getcwd()))+\"\\\\data\\\\docs_small\\\\\" # or replace with path of your own directory\n",
    "docs= os.listdir(path)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tokenize', 'this', 'sentence']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#A function to tokenize the text in very simple way\n",
    "#input: str text\n",
    "#output: list token\n",
    "def parseText(text):   \n",
    "    text= text.lower().replace(\"\\n\", \" \").replace(\",\",\" \").replace(\".\", \" \").strip()\n",
    "    tokens= text.split(\" \")\n",
    "    tokens= [i.strip() for i in tokens if i!=\"\"]\n",
    "    return tokens\n",
    "\n",
    "parseText(\"tokenize This sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'scaling': ['1.txt'], 'the': ['1.txt', '2.txt'], 'system': ['1.txt', '2.txt'], 'over': ['1.txt'], 'last': ['1.txt'], 'decade': ['1.txt'], 'search': ['1.txt'], 'engines': ['1.txt'], 'have': ['1.txt', '2.txt'], 'gone': ['1.txt'], 'from': ['1.txt'], 'crawling': ['1.txt'], 'tens': ['1.txt'], 'of': ['1.txt', '2.txt'], 'millions': ['1.txt'], 'documents': ['1.txt'], 'to': ['1.txt', '2.txt'], 'a': ['1.txt', '2.txt'], 'trillion': ['1.txt'], '[alpert': ['1.txt'], '2008]': ['1.txt'], 'building': ['1.txt'], 'an': ['1.txt', '2.txt'], 'index': ['1.txt'], 'this': ['1.txt', '2.txt'], 'scale': ['1.txt'], 'can': ['1.txt'], 'be': ['1.txt'], 'daunting': ['1.txt'], 'task': ['1.txt'], 'on': ['1.txt', '2.txt'], 'top': ['1.txt'], 'that': ['1.txt', '2.txt'], 'serving': ['1.txt'], 'hundreds': ['1.txt'], 'queries': ['1.txt'], 'per': ['1.txt'], 'second': ['1.txt'], 'against': ['1.txt'], 'such': ['1.txt', '2.txt'], 'only': ['1.txt', '2.txt'], 'makes': ['1.txt'], 'harder': ['1.txt'], 'in': ['1.txt', '2.txt'], 'mid': ['1.txt'], '2004': ['1.txt'], 'google': ['1.txt', '2.txt'], 'engine': ['1.txt'], 'processed': ['1.txt'], 'more': ['1.txt', '2.txt'], 'than': ['1.txt', '2.txt'], '200': ['1.txt'], 'million': ['1.txt'], 'day': ['1.txt'], '20tb': ['1.txt'], 'crawled': ['1.txt'], 'data': ['1.txt', '2.txt'], 'using': ['1.txt', '2.txt'], '20': ['1.txt'], '000': ['1.txt'], 'computers': ['1.txt'], '[computer': ['1.txt'], 'school': ['1.txt'], '2010]': ['1.txt'], 'web': ['1.txt'], 'use': ['1.txt', '2.txt'], 'distributed': ['1.txt', '2.txt'], 'indexing': ['1.txt'], 'algorithms': ['1.txt'], 'for': ['1.txt', '2.txt'], 'construction': ['1.txt'], 'because': ['1.txt'], 'collections': ['1.txt'], 'are': ['1.txt', '2.txt'], 'too': ['1.txt'], 'large': ['1.txt'], 'efficiently': ['1.txt'], 'single': ['1.txt'], 'machine': ['1.txt'], 'result': ['1.txt'], 'process': ['1.txt'], 'is': ['1.txt', '2.txt'], 'partitioned': ['1.txt'], 'across': ['1.txt'], 'several': ['1.txt'], 'machines': ['1.txt'], 'as': ['1.txt', '2.txt'], 'discussed': ['1.txt'], 'previous': ['1.txt'], 'section': ['1.txt'], 'according': ['1.txt'], 'term': ['1.txt'], 'or': ['1.txt', '2.txt'], 'document': ['1.txt'], 'while': ['1.txt'], 'having': ['1.txt'], 'right': ['1.txt'], 'infrastructure': ['1.txt'], 'not': ['1.txt', '2.txt'], 'necessary': ['1.txt'], '(and': ['1.txt'], 'before': ['1.txt'], 'did': ['1.txt'], 'many': ['1.txt', '2.txt'], 'these': ['1.txt'], 'pieces)': ['1.txt'], 'they': ['1.txt'], 'definitely': ['1.txt'], 'reduce': ['1.txt', '2.txt'], 'pain': ['1.txt'], 'involved': ['1.txt'], 'dependence': ['1.txt'], 'parallel': ['1.txt', '2.txt'], 'processing': ['1.txt', '2.txt'], 'increased': ['1.txt'], 'need': ['1.txt'], 'efficient': ['1.txt'], 'programming': ['1.txt', '2.txt'], 'paradigm': ['1.txt'], 'arose:': ['1.txt'], 'framework': ['1.txt', '2.txt'], 'provided': ['1.txt'], 'highest': ['1.txt'], 'computational': ['1.txt'], 'efficiency': ['1.txt'], 'maximizing': ['1.txt'], '[asanovic': ['1.txt'], 'we': ['1.txt'], 'will': ['1.txt'], 'summarize': ['1.txt'], 'features': ['1.txt', '2.txt'], 'file': ['1.txt'], 'and': ['1.txt', '2.txt'], 'map-shuffle-reduce': ['1.txt'], 'used': ['1.txt', '2.txt'], 'within': ['1.txt'], 'web-indexer': ['1.txt'], 'mapreduce': ['2.txt'], 'model': ['2.txt'], 'associated': ['2.txt'], 'implementation': ['2.txt'], 'generating': ['2.txt'], 'big': ['2.txt'], 'sets': ['2.txt'], 'with': ['2.txt'], 'algorithm': ['2.txt'], 'cluster': ['2.txt'], '[1][2][3]': ['2.txt'], 'program': ['2.txt'], 'composed': ['2.txt'], 'map': ['2.txt'], 'procedure': ['2.txt'], 'which': ['2.txt'], 'performs': ['2.txt'], 'filtering': ['2.txt'], 'sorting': ['2.txt'], '(such': ['2.txt'], 'students': ['2.txt'], 'by': ['2.txt'], 'first': ['2.txt'], 'name': ['2.txt'], 'into': ['2.txt'], 'queues': ['2.txt'], 'one': ['2.txt'], 'queue': ['2.txt'], 'each': ['2.txt'], 'name)': ['2.txt'], 'method': ['2.txt'], 'summary': ['2.txt'], 'operation': ['2.txt'], 'counting': ['2.txt'], 'number': ['2.txt'], 'yielding': ['2.txt'], 'frequencies)': ['2.txt'], '\"mapreduce': ['2.txt'], 'system\"': ['2.txt'], '(also': ['2.txt'], 'called': ['2.txt'], '\"infrastructure\"': ['2.txt'], '\"framework\")': ['2.txt'], 'orchestrates': ['2.txt'], 'marshalling': ['2.txt'], 'servers': ['2.txt'], 'running': ['2.txt'], 'various': ['2.txt'], 'tasks': ['2.txt'], 'managing': ['2.txt'], 'all': ['2.txt'], 'communications': ['2.txt'], 'transfers': ['2.txt'], 'between': ['2.txt'], 'parts': ['2.txt'], 'providing': ['2.txt'], 'redundancy': ['2.txt'], 'fault': ['2.txt'], 'tolerance': ['2.txt'], 'specialization': ['2.txt'], 'split-apply-combine': ['2.txt'], 'strategy': ['2.txt'], 'analysis': ['2.txt'], '[4]': ['2.txt'], 'it': ['2.txt'], 'inspired': ['2.txt'], 'functions': ['2.txt'], 'commonly': ['2.txt'], 'functional': ['2.txt'], '[5]': ['2.txt'], 'although': ['2.txt'], 'their': ['2.txt'], 'purpose': ['2.txt'], 'same': ['2.txt'], 'original': ['2.txt'], 'forms': ['2.txt'], '[6]': ['2.txt'], 'key': ['2.txt'], 'contributions': ['2.txt'], 'actual': ['2.txt'], '(which': ['2.txt'], 'example': ['2.txt'], 'resemble': ['2.txt'], '1995': ['2.txt'], 'message': ['2.txt'], 'passing': ['2.txt'], 'interface': ['2.txt'], \"standard's[7]\": ['2.txt'], 'reduce[8]': ['2.txt'], 'scatter[9]': ['2.txt'], 'operations)': ['2.txt'], 'but': ['2.txt'], 'scalability': ['2.txt'], 'fault-tolerance': ['2.txt'], 'achieved': ['2.txt'], 'variety': ['2.txt'], 'applications': ['2.txt'], 'optimizing': ['2.txt'], 'execution': ['2.txt'], 'engine[citation': ['2.txt'], 'needed]': ['2.txt'], 'single-threaded': ['2.txt'], 'usually': ['2.txt'], 'faster': ['2.txt'], 'traditional': ['2.txt'], '(non-mapreduce)': ['2.txt'], 'implementation;': ['2.txt'], 'any': ['2.txt'], 'gains': ['2.txt'], 'seen': ['2.txt'], 'multi-threaded': ['2.txt'], 'implementations': ['2.txt'], 'multi-processor': ['2.txt'], 'hardware': ['2.txt'], '[10]': ['2.txt'], 'beneficial': ['2.txt'], 'when': ['2.txt'], 'optimized': ['2.txt'], 'shuffle': ['2.txt'], 'reduces': ['2.txt'], 'network': ['2.txt'], 'communication': ['2.txt'], 'cost)': ['2.txt'], 'come': ['2.txt'], 'play': ['2.txt'], 'cost': ['2.txt'], 'essential': ['2.txt'], 'good': ['2.txt'], '[11]': ['2.txt'], 'libraries': ['2.txt'], 'been': ['2.txt'], 'written': ['2.txt'], 'languages': ['2.txt'], 'different': ['2.txt'], 'levels': ['2.txt'], 'optimization': ['2.txt'], 'popular': ['2.txt'], 'open-source': ['2.txt'], 'has': ['2.txt'], 'support': ['2.txt'], 'shuffles': ['2.txt'], 'part': ['2.txt'], 'apache': ['2.txt'], 'hadoop': ['2.txt'], 'originally': ['2.txt'], 'referred': ['2.txt'], 'proprietary': ['2.txt'], 'technology': ['2.txt'], 'since': ['2.txt'], 'genericized': ['2.txt'], '2014': ['2.txt'], 'was': ['2.txt'], 'no': ['2.txt'], 'longer': ['2.txt'], 'primary': ['2.txt'], '[12]': ['2.txt'], 'development': ['2.txt'], 'mahout': ['2.txt'], 'had': ['2.txt'], 'moved': ['2.txt'], 'capable': ['2.txt'], 'less': ['2.txt'], 'disk-oriented': ['2.txt'], 'mechanisms': ['2.txt'], 'incorporated': ['2.txt'], 'full': ['2.txt'], 'capabilities': ['2.txt'], '[13]': ['2.txt']}\n"
     ]
    }
   ],
   "source": [
    "# keeping index in dict\n",
    "index={} # keys are terms and values are postings, where postings are list of document\n",
    "# parse each document and add token to dictonary\n",
    "for doc in docs: \n",
    "    dpath=(os.path.join(path, doc))\n",
    "    text=open(dpath).read()\n",
    "    tokens= parseText(text) \n",
    "    \n",
    "    for token in tokens:\n",
    "        posting= index.get(token, None)\n",
    "        \n",
    "        if posting is None:      \n",
    "            posting= [doc]\n",
    "        else:    \n",
    "            if doc not in posting: # You can also keep TF here\n",
    "                posting.append(doc) \n",
    "        index[token]= posting\n",
    "       \n",
    "print(index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\"framework\")': ['2.txt'],\n",
       " '\"infrastructure\"': ['2.txt'],\n",
       " '\"mapreduce': ['2.txt'],\n",
       " '(also': ['2.txt'],\n",
       " '(and': ['1.txt'],\n",
       " '(non-mapreduce)': ['2.txt'],\n",
       " '(such': ['2.txt'],\n",
       " '(which': ['2.txt'],\n",
       " '000': ['1.txt'],\n",
       " '1995': ['2.txt'],\n",
       " '20': ['1.txt'],\n",
       " '200': ['1.txt'],\n",
       " '2004': ['1.txt'],\n",
       " '2008]': ['1.txt'],\n",
       " '2010]': ['1.txt'],\n",
       " '2014': ['2.txt'],\n",
       " '20tb': ['1.txt'],\n",
       " '[10]': ['2.txt'],\n",
       " '[11]': ['2.txt'],\n",
       " '[12]': ['2.txt'],\n",
       " '[13]': ['2.txt'],\n",
       " '[1][2][3]': ['2.txt'],\n",
       " '[4]': ['2.txt'],\n",
       " '[5]': ['2.txt'],\n",
       " '[6]': ['2.txt'],\n",
       " '[alpert': ['1.txt'],\n",
       " '[asanovic': ['1.txt'],\n",
       " '[computer': ['1.txt'],\n",
       " 'a': ['1.txt', '2.txt'],\n",
       " 'according': ['1.txt'],\n",
       " 'achieved': ['2.txt'],\n",
       " 'across': ['1.txt'],\n",
       " 'actual': ['2.txt'],\n",
       " 'against': ['1.txt'],\n",
       " 'algorithm': ['2.txt'],\n",
       " 'algorithms': ['1.txt'],\n",
       " 'all': ['2.txt'],\n",
       " 'although': ['2.txt'],\n",
       " 'an': ['1.txt', '2.txt'],\n",
       " 'analysis': ['2.txt'],\n",
       " 'and': ['1.txt', '2.txt'],\n",
       " 'any': ['2.txt'],\n",
       " 'apache': ['2.txt'],\n",
       " 'applications': ['2.txt'],\n",
       " 'are': ['1.txt', '2.txt'],\n",
       " 'arose:': ['1.txt'],\n",
       " 'as': ['1.txt', '2.txt'],\n",
       " 'associated': ['2.txt'],\n",
       " 'be': ['1.txt'],\n",
       " 'because': ['1.txt'],\n",
       " 'been': ['2.txt'],\n",
       " 'before': ['1.txt'],\n",
       " 'beneficial': ['2.txt'],\n",
       " 'between': ['2.txt'],\n",
       " 'big': ['2.txt'],\n",
       " 'building': ['1.txt'],\n",
       " 'but': ['2.txt'],\n",
       " 'by': ['2.txt'],\n",
       " 'called': ['2.txt'],\n",
       " 'can': ['1.txt'],\n",
       " 'capabilities': ['2.txt'],\n",
       " 'capable': ['2.txt'],\n",
       " 'cluster': ['2.txt'],\n",
       " 'collections': ['1.txt'],\n",
       " 'come': ['2.txt'],\n",
       " 'commonly': ['2.txt'],\n",
       " 'communication': ['2.txt'],\n",
       " 'communications': ['2.txt'],\n",
       " 'composed': ['2.txt'],\n",
       " 'computational': ['1.txt'],\n",
       " 'computers': ['1.txt'],\n",
       " 'construction': ['1.txt'],\n",
       " 'contributions': ['2.txt'],\n",
       " 'cost': ['2.txt'],\n",
       " 'cost)': ['2.txt'],\n",
       " 'counting': ['2.txt'],\n",
       " 'crawled': ['1.txt'],\n",
       " 'crawling': ['1.txt'],\n",
       " 'data': ['1.txt', '2.txt'],\n",
       " 'daunting': ['1.txt'],\n",
       " 'day': ['1.txt'],\n",
       " 'decade': ['1.txt'],\n",
       " 'definitely': ['1.txt'],\n",
       " 'dependence': ['1.txt'],\n",
       " 'development': ['2.txt'],\n",
       " 'did': ['1.txt'],\n",
       " 'different': ['2.txt'],\n",
       " 'discussed': ['1.txt'],\n",
       " 'disk-oriented': ['2.txt'],\n",
       " 'distributed': ['1.txt', '2.txt'],\n",
       " 'document': ['1.txt'],\n",
       " 'documents': ['1.txt'],\n",
       " 'each': ['2.txt'],\n",
       " 'efficiency': ['1.txt'],\n",
       " 'efficient': ['1.txt'],\n",
       " 'efficiently': ['1.txt'],\n",
       " 'engine': ['1.txt'],\n",
       " 'engine[citation': ['2.txt'],\n",
       " 'engines': ['1.txt'],\n",
       " 'essential': ['2.txt'],\n",
       " 'example': ['2.txt'],\n",
       " 'execution': ['2.txt'],\n",
       " 'faster': ['2.txt'],\n",
       " 'fault': ['2.txt'],\n",
       " 'fault-tolerance': ['2.txt'],\n",
       " 'features': ['1.txt', '2.txt'],\n",
       " 'file': ['1.txt'],\n",
       " 'filtering': ['2.txt'],\n",
       " 'first': ['2.txt'],\n",
       " 'for': ['1.txt', '2.txt'],\n",
       " 'forms': ['2.txt'],\n",
       " 'framework': ['1.txt', '2.txt'],\n",
       " 'frequencies)': ['2.txt'],\n",
       " 'from': ['1.txt'],\n",
       " 'full': ['2.txt'],\n",
       " 'functional': ['2.txt'],\n",
       " 'functions': ['2.txt'],\n",
       " 'gains': ['2.txt'],\n",
       " 'generating': ['2.txt'],\n",
       " 'genericized': ['2.txt'],\n",
       " 'gone': ['1.txt'],\n",
       " 'good': ['2.txt'],\n",
       " 'google': ['1.txt', '2.txt'],\n",
       " 'had': ['2.txt'],\n",
       " 'hadoop': ['2.txt'],\n",
       " 'harder': ['1.txt'],\n",
       " 'hardware': ['2.txt'],\n",
       " 'has': ['2.txt'],\n",
       " 'have': ['1.txt', '2.txt'],\n",
       " 'having': ['1.txt'],\n",
       " 'highest': ['1.txt'],\n",
       " 'hundreds': ['1.txt'],\n",
       " 'implementation': ['2.txt'],\n",
       " 'implementation;': ['2.txt'],\n",
       " 'implementations': ['2.txt'],\n",
       " 'in': ['1.txt', '2.txt'],\n",
       " 'incorporated': ['2.txt'],\n",
       " 'increased': ['1.txt'],\n",
       " 'index': ['1.txt'],\n",
       " 'indexing': ['1.txt'],\n",
       " 'infrastructure': ['1.txt'],\n",
       " 'inspired': ['2.txt'],\n",
       " 'interface': ['2.txt'],\n",
       " 'into': ['2.txt'],\n",
       " 'involved': ['1.txt'],\n",
       " 'is': ['1.txt', '2.txt'],\n",
       " 'it': ['2.txt'],\n",
       " 'key': ['2.txt'],\n",
       " 'languages': ['2.txt'],\n",
       " 'large': ['1.txt'],\n",
       " 'last': ['1.txt'],\n",
       " 'less': ['2.txt'],\n",
       " 'levels': ['2.txt'],\n",
       " 'libraries': ['2.txt'],\n",
       " 'longer': ['2.txt'],\n",
       " 'machine': ['1.txt'],\n",
       " 'machines': ['1.txt'],\n",
       " 'mahout': ['2.txt'],\n",
       " 'makes': ['1.txt'],\n",
       " 'managing': ['2.txt'],\n",
       " 'many': ['1.txt', '2.txt'],\n",
       " 'map': ['2.txt'],\n",
       " 'map-shuffle-reduce': ['1.txt'],\n",
       " 'mapreduce': ['2.txt'],\n",
       " 'marshalling': ['2.txt'],\n",
       " 'maximizing': ['1.txt'],\n",
       " 'mechanisms': ['2.txt'],\n",
       " 'message': ['2.txt'],\n",
       " 'method': ['2.txt'],\n",
       " 'mid': ['1.txt'],\n",
       " 'million': ['1.txt'],\n",
       " 'millions': ['1.txt'],\n",
       " 'model': ['2.txt'],\n",
       " 'more': ['1.txt', '2.txt'],\n",
       " 'moved': ['2.txt'],\n",
       " 'multi-processor': ['2.txt'],\n",
       " 'multi-threaded': ['2.txt'],\n",
       " 'name': ['2.txt'],\n",
       " 'name)': ['2.txt'],\n",
       " 'necessary': ['1.txt'],\n",
       " 'need': ['1.txt'],\n",
       " 'needed]': ['2.txt'],\n",
       " 'network': ['2.txt'],\n",
       " 'no': ['2.txt'],\n",
       " 'not': ['1.txt', '2.txt'],\n",
       " 'number': ['2.txt'],\n",
       " 'of': ['1.txt', '2.txt'],\n",
       " 'on': ['1.txt', '2.txt'],\n",
       " 'one': ['2.txt'],\n",
       " 'only': ['1.txt', '2.txt'],\n",
       " 'open-source': ['2.txt'],\n",
       " 'operation': ['2.txt'],\n",
       " 'operations)': ['2.txt'],\n",
       " 'optimization': ['2.txt'],\n",
       " 'optimized': ['2.txt'],\n",
       " 'optimizing': ['2.txt'],\n",
       " 'or': ['1.txt', '2.txt'],\n",
       " 'orchestrates': ['2.txt'],\n",
       " 'original': ['2.txt'],\n",
       " 'originally': ['2.txt'],\n",
       " 'over': ['1.txt'],\n",
       " 'pain': ['1.txt'],\n",
       " 'paradigm': ['1.txt'],\n",
       " 'parallel': ['1.txt', '2.txt'],\n",
       " 'part': ['2.txt'],\n",
       " 'partitioned': ['1.txt'],\n",
       " 'parts': ['2.txt'],\n",
       " 'passing': ['2.txt'],\n",
       " 'per': ['1.txt'],\n",
       " 'performs': ['2.txt'],\n",
       " 'pieces)': ['1.txt'],\n",
       " 'play': ['2.txt'],\n",
       " 'popular': ['2.txt'],\n",
       " 'previous': ['1.txt'],\n",
       " 'primary': ['2.txt'],\n",
       " 'procedure': ['2.txt'],\n",
       " 'process': ['1.txt'],\n",
       " 'processed': ['1.txt'],\n",
       " 'processing': ['1.txt', '2.txt'],\n",
       " 'program': ['2.txt'],\n",
       " 'programming': ['1.txt', '2.txt'],\n",
       " 'proprietary': ['2.txt'],\n",
       " 'provided': ['1.txt'],\n",
       " 'providing': ['2.txt'],\n",
       " 'purpose': ['2.txt'],\n",
       " 'queries': ['1.txt'],\n",
       " 'queue': ['2.txt'],\n",
       " 'queues': ['2.txt'],\n",
       " 'reduce': ['1.txt', '2.txt'],\n",
       " 'reduce[8]': ['2.txt'],\n",
       " 'reduces': ['2.txt'],\n",
       " 'redundancy': ['2.txt'],\n",
       " 'referred': ['2.txt'],\n",
       " 'resemble': ['2.txt'],\n",
       " 'result': ['1.txt'],\n",
       " 'right': ['1.txt'],\n",
       " 'running': ['2.txt'],\n",
       " 'same': ['2.txt'],\n",
       " 'scalability': ['2.txt'],\n",
       " 'scale': ['1.txt'],\n",
       " 'scaling': ['1.txt'],\n",
       " 'scatter[9]': ['2.txt'],\n",
       " 'school': ['1.txt'],\n",
       " 'search': ['1.txt'],\n",
       " 'second': ['1.txt'],\n",
       " 'section': ['1.txt'],\n",
       " 'seen': ['2.txt'],\n",
       " 'servers': ['2.txt'],\n",
       " 'serving': ['1.txt'],\n",
       " 'sets': ['2.txt'],\n",
       " 'several': ['1.txt'],\n",
       " 'shuffle': ['2.txt'],\n",
       " 'shuffles': ['2.txt'],\n",
       " 'since': ['2.txt'],\n",
       " 'single': ['1.txt'],\n",
       " 'single-threaded': ['2.txt'],\n",
       " 'sorting': ['2.txt'],\n",
       " 'specialization': ['2.txt'],\n",
       " 'split-apply-combine': ['2.txt'],\n",
       " \"standard's[7]\": ['2.txt'],\n",
       " 'strategy': ['2.txt'],\n",
       " 'students': ['2.txt'],\n",
       " 'such': ['1.txt', '2.txt'],\n",
       " 'summarize': ['1.txt'],\n",
       " 'summary': ['2.txt'],\n",
       " 'support': ['2.txt'],\n",
       " 'system': ['1.txt', '2.txt'],\n",
       " 'system\"': ['2.txt'],\n",
       " 'task': ['1.txt'],\n",
       " 'tasks': ['2.txt'],\n",
       " 'technology': ['2.txt'],\n",
       " 'tens': ['1.txt'],\n",
       " 'term': ['1.txt'],\n",
       " 'than': ['1.txt', '2.txt'],\n",
       " 'that': ['1.txt', '2.txt'],\n",
       " 'the': ['1.txt', '2.txt'],\n",
       " 'their': ['2.txt'],\n",
       " 'these': ['1.txt'],\n",
       " 'they': ['1.txt'],\n",
       " 'this': ['1.txt', '2.txt'],\n",
       " 'to': ['1.txt', '2.txt'],\n",
       " 'tolerance': ['2.txt'],\n",
       " 'too': ['1.txt'],\n",
       " 'top': ['1.txt'],\n",
       " 'traditional': ['2.txt'],\n",
       " 'transfers': ['2.txt'],\n",
       " 'trillion': ['1.txt'],\n",
       " 'use': ['1.txt', '2.txt'],\n",
       " 'used': ['1.txt', '2.txt'],\n",
       " 'using': ['1.txt', '2.txt'],\n",
       " 'usually': ['2.txt'],\n",
       " 'variety': ['2.txt'],\n",
       " 'various': ['2.txt'],\n",
       " 'was': ['2.txt'],\n",
       " 'we': ['1.txt'],\n",
       " 'web': ['1.txt'],\n",
       " 'web-indexer': ['1.txt'],\n",
       " 'when': ['2.txt'],\n",
       " 'which': ['2.txt'],\n",
       " 'while': ['1.txt'],\n",
       " 'will': ['1.txt'],\n",
       " 'with': ['2.txt'],\n",
       " 'within': ['1.txt'],\n",
       " 'written': ['2.txt'],\n",
       " 'yielding': ['2.txt']}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(sorted(index.items()))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'scaling': {'1.txt': 1}, 'the': {'1.txt': 14, '2.txt': 24}, 'system': {'1.txt': 4, '2.txt': 1}, 'over': {'1.txt': 2}, 'last': {'1.txt': 1}, 'decade': {'1.txt': 1}, 'search': {'1.txt': 4}, 'engines': {'1.txt': 3}, 'have': {'1.txt': 2, '2.txt': 1}, 'gone': {'1.txt': 1}, 'from': {'1.txt': 1}, 'crawling': {'1.txt': 1}, 'tens': {'1.txt': 1}, 'of': {'1.txt': 9, '2.txt': 11}, 'millions': {'1.txt': 1}, 'documents': {'1.txt': 2}, 'to': {'1.txt': 4, '2.txt': 3}, 'a': {'1.txt': 10, '2.txt': 13}, 'trillion': {'1.txt': 1}, '[alpert': {'1.txt': 1}, '2008]': {'1.txt': 2}, 'building': {'1.txt': 2}, 'an': {'1.txt': 3, '2.txt': 1}, 'index': {'1.txt': 5}, 'this': {'1.txt': 3, '2.txt': 1}, 'scale': {'1.txt': 1}, 'can': {'1.txt': 3}, 'be': {'1.txt': 2}, 'daunting': {'1.txt': 1}, 'task': {'1.txt': 2}, 'on': {'1.txt': 3, '2.txt': 4}, 'top': {'1.txt': 1}, 'that': {'1.txt': 3, '2.txt': 2}, 'serving': {'1.txt': 1}, 'hundreds': {'1.txt': 1}, 'queries': {'1.txt': 2}, 'per': {'1.txt': 1}, 'second': {'1.txt': 1}, 'against': {'1.txt': 2}, 'such': {'1.txt': 2, '2.txt': 1}, 'only': {'1.txt': 1, '2.txt': 2}, 'makes': {'1.txt': 1}, 'harder': {'1.txt': 1}, 'in': {'1.txt': 4, '2.txt': 6}, 'mid': {'1.txt': 1}, '2004': {'1.txt': 2}, 'google': {'1.txt': 1, '2.txt': 2}, 'engine': {'1.txt': 1}, 'processed': {'1.txt': 2}, 'more': {'1.txt': 3, '2.txt': 1}, 'than': {'1.txt': 3, '2.txt': 1}, '200': {'1.txt': 1}, 'million': {'1.txt': 1}, 'day': {'1.txt': 1}, '20tb': {'1.txt': 1}, 'crawled': {'1.txt': 1}, 'data': {'1.txt': 2, '2.txt': 4}, 'using': {'1.txt': 1, '2.txt': 1}, '20': {'1.txt': 1}, '000': {'1.txt': 1}, 'computers': {'1.txt': 1}, '[computer': {'1.txt': 1}, 'school': {'1.txt': 1}, '2010]': {'1.txt': 1}, 'web': {'1.txt': 1}, 'use': {'1.txt': 1, '2.txt': 1}, 'distributed': {'1.txt': 4, '2.txt': 4}, 'indexing': {'1.txt': 1}, 'algorithms': {'1.txt': 1}, 'for': {'1.txt': 2, '2.txt': 7}, 'construction': {'1.txt': 2}, 'because': {'1.txt': 1}, 'collections': {'1.txt': 1}, 'are': {'1.txt': 1, '2.txt': 2}, 'too': {'1.txt': 1}, 'large': {'1.txt': 1}, 'efficiently': {'1.txt': 1}, 'single': {'1.txt': 1}, 'machine': {'1.txt': 1}, 'result': {'1.txt': 1}, 'process': {'1.txt': 1}, 'is': {'1.txt': 3, '2.txt': 9}, 'partitioned': {'1.txt': 2}, 'across': {'1.txt': 1}, 'several': {'1.txt': 1}, 'machines': {'1.txt': 1}, 'as': {'1.txt': 2, '2.txt': 5}, 'discussed': {'1.txt': 1}, 'previous': {'1.txt': 1}, 'section': {'1.txt': 2}, 'according': {'1.txt': 2}, 'term': {'1.txt': 1}, 'or': {'1.txt': 1, '2.txt': 1}, 'document': {'1.txt': 1}, 'while': {'1.txt': 2}, 'having': {'1.txt': 1}, 'right': {'1.txt': 1}, 'infrastructure': {'1.txt': 1}, 'not': {'1.txt': 2, '2.txt': 3}, 'necessary': {'1.txt': 1}, '(and': {'1.txt': 1}, 'before': {'1.txt': 1}, 'did': {'1.txt': 1}, 'many': {'1.txt': 1, '2.txt': 1}, 'these': {'1.txt': 1}, 'pieces)': {'1.txt': 1}, 'they': {'1.txt': 1}, 'definitely': {'1.txt': 1}, 'reduce': {'1.txt': 1, '2.txt': 4}, 'pain': {'1.txt': 1}, 'involved': {'1.txt': 1}, 'dependence': {'1.txt': 1}, 'parallel': {'1.txt': 2, '2.txt': 2}, 'processing': {'1.txt': 1, '2.txt': 3}, 'increased': {'1.txt': 1}, 'need': {'1.txt': 1}, 'efficient': {'1.txt': 1}, 'programming': {'1.txt': 2, '2.txt': 3}, 'paradigm': {'1.txt': 1}, 'arose:': {'1.txt': 1}, 'framework': {'1.txt': 1, '2.txt': 3}, 'provided': {'1.txt': 1}, 'highest': {'1.txt': 1}, 'computational': {'1.txt': 1}, 'efficiency': {'1.txt': 2}, 'maximizing': {'1.txt': 1}, '[asanovic': {'1.txt': 1}, 'we': {'1.txt': 1}, 'will': {'1.txt': 1}, 'summarize': {'1.txt': 1}, 'features': {'1.txt': 1, '2.txt': 1}, 'file': {'1.txt': 1}, 'and': {'1.txt': 1, '2.txt': 15}, 'map-shuffle-reduce': {'1.txt': 1}, 'used': {'1.txt': 1, '2.txt': 1}, 'within': {'1.txt': 1}, 'web-indexer': {'1.txt': 1}, 'mapreduce': {'2.txt': 10}, 'model': {'2.txt': 4}, 'associated': {'2.txt': 1}, 'implementation': {'2.txt': 3}, 'generating': {'2.txt': 1}, 'big': {'2.txt': 2}, 'sets': {'2.txt': 1}, 'with': {'2.txt': 3}, 'algorithm': {'2.txt': 2}, 'cluster': {'2.txt': 1}, '[1][2][3]': {'2.txt': 1}, 'program': {'2.txt': 1}, 'composed': {'2.txt': 1}, 'map': {'2.txt': 4}, 'procedure': {'2.txt': 1}, 'which': {'2.txt': 2}, 'performs': {'2.txt': 2}, 'filtering': {'2.txt': 1}, 'sorting': {'2.txt': 2}, '(such': {'2.txt': 2}, 'students': {'2.txt': 2}, 'by': {'2.txt': 5}, 'first': {'2.txt': 1}, 'name': {'2.txt': 3}, 'into': {'2.txt': 2}, 'queues': {'2.txt': 1}, 'one': {'2.txt': 1}, 'queue': {'2.txt': 2}, 'each': {'2.txt': 2}, 'name)': {'2.txt': 1}, 'method': {'2.txt': 1}, 'summary': {'2.txt': 1}, 'operation': {'2.txt': 2}, 'counting': {'2.txt': 1}, 'number': {'2.txt': 1}, 'yielding': {'2.txt': 1}, 'frequencies)': {'2.txt': 1}, '\"mapreduce': {'2.txt': 1}, 'system\"': {'2.txt': 1}, '(also': {'2.txt': 1}, 'called': {'2.txt': 1}, '\"infrastructure\"': {'2.txt': 1}, '\"framework\")': {'2.txt': 1}, 'orchestrates': {'2.txt': 1}, 'marshalling': {'2.txt': 1}, 'servers': {'2.txt': 1}, 'running': {'2.txt': 1}, 'various': {'2.txt': 2}, 'tasks': {'2.txt': 1}, 'managing': {'2.txt': 1}, 'all': {'2.txt': 1}, 'communications': {'2.txt': 1}, 'transfers': {'2.txt': 1}, 'between': {'2.txt': 1}, 'parts': {'2.txt': 1}, 'providing': {'2.txt': 1}, 'redundancy': {'2.txt': 1}, 'fault': {'2.txt': 2}, 'tolerance': {'2.txt': 2}, 'specialization': {'2.txt': 1}, 'split-apply-combine': {'2.txt': 1}, 'strategy': {'2.txt': 1}, 'analysis': {'2.txt': 1}, '[4]': {'2.txt': 1}, 'it': {'2.txt': 1}, 'inspired': {'2.txt': 1}, 'functions': {'2.txt': 2}, 'commonly': {'2.txt': 1}, 'functional': {'2.txt': 1}, '[5]': {'2.txt': 1}, 'although': {'2.txt': 1}, 'their': {'2.txt': 3}, 'purpose': {'2.txt': 1}, 'same': {'2.txt': 1}, 'original': {'2.txt': 1}, 'forms': {'2.txt': 1}, '[6]': {'2.txt': 1}, 'key': {'2.txt': 1}, 'contributions': {'2.txt': 1}, 'actual': {'2.txt': 1}, '(which': {'2.txt': 2}, 'example': {'2.txt': 1}, 'resemble': {'2.txt': 1}, '1995': {'2.txt': 1}, 'message': {'2.txt': 1}, 'passing': {'2.txt': 1}, 'interface': {'2.txt': 1}, \"standard's[7]\": {'2.txt': 1}, 'reduce[8]': {'2.txt': 1}, 'scatter[9]': {'2.txt': 1}, 'operations)': {'2.txt': 1}, 'but': {'2.txt': 2}, 'scalability': {'2.txt': 1}, 'fault-tolerance': {'2.txt': 1}, 'achieved': {'2.txt': 1}, 'variety': {'2.txt': 1}, 'applications': {'2.txt': 1}, 'optimizing': {'2.txt': 2}, 'execution': {'2.txt': 1}, 'engine[citation': {'2.txt': 1}, 'needed]': {'2.txt': 1}, 'single-threaded': {'2.txt': 1}, 'usually': {'2.txt': 2}, 'faster': {'2.txt': 1}, 'traditional': {'2.txt': 1}, '(non-mapreduce)': {'2.txt': 1}, 'implementation;': {'2.txt': 1}, 'any': {'2.txt': 1}, 'gains': {'2.txt': 1}, 'seen': {'2.txt': 1}, 'multi-threaded': {'2.txt': 1}, 'implementations': {'2.txt': 1}, 'multi-processor': {'2.txt': 1}, 'hardware': {'2.txt': 1}, '[10]': {'2.txt': 1}, 'beneficial': {'2.txt': 1}, 'when': {'2.txt': 1}, 'optimized': {'2.txt': 1}, 'shuffle': {'2.txt': 1}, 'reduces': {'2.txt': 1}, 'network': {'2.txt': 1}, 'communication': {'2.txt': 2}, 'cost)': {'2.txt': 1}, 'come': {'2.txt': 1}, 'play': {'2.txt': 1}, 'cost': {'2.txt': 1}, 'essential': {'2.txt': 1}, 'good': {'2.txt': 1}, '[11]': {'2.txt': 1}, 'libraries': {'2.txt': 1}, 'been': {'2.txt': 2}, 'written': {'2.txt': 1}, 'languages': {'2.txt': 1}, 'different': {'2.txt': 1}, 'levels': {'2.txt': 1}, 'optimization': {'2.txt': 1}, 'popular': {'2.txt': 1}, 'open-source': {'2.txt': 1}, 'has': {'2.txt': 2}, 'support': {'2.txt': 1}, 'shuffles': {'2.txt': 1}, 'part': {'2.txt': 1}, 'apache': {'2.txt': 2}, 'hadoop': {'2.txt': 1}, 'originally': {'2.txt': 1}, 'referred': {'2.txt': 1}, 'proprietary': {'2.txt': 1}, 'technology': {'2.txt': 1}, 'since': {'2.txt': 1}, 'genericized': {'2.txt': 1}, '2014': {'2.txt': 1}, 'was': {'2.txt': 1}, 'no': {'2.txt': 1}, 'longer': {'2.txt': 1}, 'primary': {'2.txt': 1}, '[12]': {'2.txt': 1}, 'development': {'2.txt': 1}, 'mahout': {'2.txt': 1}, 'had': {'2.txt': 1}, 'moved': {'2.txt': 1}, 'capable': {'2.txt': 1}, 'less': {'2.txt': 1}, 'disk-oriented': {'2.txt': 1}, 'mechanisms': {'2.txt': 1}, 'incorporated': {'2.txt': 1}, 'full': {'2.txt': 1}, 'capabilities': {'2.txt': 1}, '[13]': {'2.txt': 1}}\n"
     ]
    }
   ],
   "source": [
    "# index keeping term frequency aswell\n",
    "# keeping index in dict\n",
    "index={} # keys are terms and values are postings, \n",
    "# where a posting is dict  of <docID, freq>\n",
    "\n",
    "# parse each document and add token to dictonary\n",
    "for doc in docs: \n",
    "    dpath=(os.path.join(path, doc))\n",
    "    text=open(dpath).read()\n",
    "    tokens= parseText(text) \n",
    "\n",
    "    # if index size is >=some value\n",
    "    for token in tokens:\n",
    "        posting= index.get(token, None)\n",
    "        \n",
    "        if posting is None:      \n",
    "            posting= {doc:1}\n",
    "        else:    \n",
    "            if doc not in posting: # You can also keep TF here\n",
    "                posting[doc]= 1\n",
    "            else:\n",
    "                posting[doc]= posting[doc]+1\n",
    "        index[token]= posting\n",
    "       \n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
